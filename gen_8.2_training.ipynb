{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yatzy - Deep RL - Gen 8.2\n",
    "##### Uses 3 networks as opposed to one; provides improvements over Gen 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import math\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from player import Player\n",
    "from hand import YatzyHand\n",
    "\n",
    "from ai_gen_8 import AIGenEightPointOne\n",
    "from deeprlgame2 import DeepRLGame3DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions, Replay Memories and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovesTransition = namedtuple('MoveTransition', \n",
    "                            ('state', 'move', 'reward'))\n",
    "RerollTransition = namedtuple('RerollTransition',\n",
    "                            ('state', 'reroll', 'next_state'))\n",
    "IndicesTransition = namedtuple('IndicesTransition', \n",
    "                            ('state', 'indices', 'next_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovesReplayMemory(ReplayMemory):\n",
    "    def push(self, *args):\n",
    "        self.memory.append(MovesTransition(*args))\n",
    "\n",
    "class IndicesReplayMemory(ReplayMemory):\n",
    "    def push(self, *args):\n",
    "        self.memory.append(IndicesTransition(*args))\n",
    "\n",
    "class RerollReplayMemory(ReplayMemory):\n",
    "    def push(self, *args):\n",
    "        self.memory.append(RerollTransition(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, output):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(20, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state(scoresheet, hand):\n",
    "    s = []\n",
    "\n",
    "    for key, value in scoresheet.items():\n",
    "        if not isinstance(value, int):\n",
    "            s.append(1)\n",
    "        else:\n",
    "            s.append(value * 5)\n",
    "\n",
    "    for die in hand:\n",
    "        s.append(float(die) * 100)\n",
    "\n",
    "    return torch.tensor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_state_end(state):\n",
    "    state = list(state)\n",
    "    return all([x != 1 for x in state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_dicts():\n",
    "    moves = {0: 'ones', 1: 'twos', 2: 'threes', 3: 'fours', 4: 'fives',\n",
    "  5: 'sixes', 6: 'one_pair', 7: 'two_pair', 8: 'three_kind', 9: 'four_kind', 10: 'small_straight',\n",
    "  11: 'large_straight', 12: 'full_house', 13: 'chance', 14: 'yatzy'}\n",
    "\n",
    "    reroll = {0: 'True', 1: 'False'}\n",
    "\n",
    "    indices = {0: (0,), 1: (1,), 2: (2,), 3: (3,), 4: (4,), 5: (0, 1), 6: (0, 2),\n",
    "  7: (0, 3), 8: (0, 4), 9: (1, 2), 10: (1, 3), 11: (1, 4), 12: (2, 3), 13: (2, 4),\n",
    "  14: (3, 4), 15: (0, 1, 2), 16: (0, 1, 3), 17: (0, 1, 4), 18: (0, 2, 3), 19: (0, 2, 4), \n",
    "  20: (0, 3, 4), 21: (1, 2, 3), 22: (1, 2, 4), 23: (1, 3, 4), 24: (2, 3, 4), 25: (0, 1, 2, 3),\n",
    "  26: (0, 1, 2, 4), 27: (0, 1, 3, 4), 28: (0, 2, 3, 4), 29: (1, 2, 3, 4), 30: (0, 1, 2, 3, 4)}\n",
    "\n",
    "    \n",
    "    return moves, reroll, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_moves(scoresheet):\n",
    "    available = {}\n",
    "    for i, (key, value) in enumerate(scoresheet.items()):\n",
    "        if value is not None:\n",
    "            available[i] = False\n",
    "        else:\n",
    "            available[i] = True\n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_move(state, available_moves, episode_num, eps=True):\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_final + (eps_initial - eps_final) * math.exp(-1.0 * episode_num / eps_decay)\n",
    "\n",
    "    if sample < eps_threshold and eps:\n",
    "        global total_random\n",
    "        total_random += 1\n",
    "        return torch.tensor(random.choice([x for x in available_moves if available_moves[x]])).view(1).to(device)\n",
    "    else:\n",
    "        global total_nonrand \n",
    "        total_nonrand += 1\n",
    "        with torch.no_grad():\n",
    "            results = moves_target(state)\n",
    "            for i in range(15):\n",
    "                if available_moves[i] == False:\n",
    "                    results[i] = 0\n",
    "            return torch.argmax(results).view(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_reroll(state, available_rerolls, episode_num, eps=True):\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_final + (eps_initial - eps_final) * math.exp(-1.0 * episode_num / eps_decay)\n",
    "\n",
    "    if sample < eps_threshold and eps:\n",
    "        global total_random\n",
    "        total_random += 1\n",
    "        return torch.tensor(random.choice([x for x in available_rerolls if available_rerolls[x]])).view(1).to(device)\n",
    "    else:\n",
    "        global total_nonrand \n",
    "        total_nonrand += 1\n",
    "        with torch.no_grad():\n",
    "            results = reroll_target(state)\n",
    "            return torch.argmax(results).view(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_indices(state, available_indices, episode_num, eps=True):\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_final + (eps_initial - eps_final) * math.exp(-1.0 * episode_num / eps_decay)\n",
    "\n",
    "    if sample < eps_threshold and eps:\n",
    "        global total_random\n",
    "        total_random += 1\n",
    "        return torch.tensor(random.choice([x for x in available_indices if available_indices[x]])).view(1).to(device)\n",
    "    else:\n",
    "        global total_nonrand \n",
    "        total_nonrand += 1\n",
    "        with torch.no_grad():\n",
    "            results = indices_target(state)\n",
    "            return torch.argmax(results).view(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6823819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_moves():\n",
    "    if len(moves_memory) < bs:\n",
    "        return\n",
    "    transitions = moves_memory.sample(bs)\n",
    "\n",
    "    batch = MovesTransition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.move)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = moves_policy(state_batch).gather(1, action_batch).squeeze()\n",
    "    \n",
    "    expected_state_action_values = reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    moves_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    moves_opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_reroll():\n",
    "    if len(reroll_memory) < bs:\n",
    "        return\n",
    "    transitions = reroll_memory.sample(bs)\n",
    "\n",
    "    batch = RerollTransition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: not check_state_end(s), batch.next_state)), device=device, dtype = torch.bool)\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.reroll)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state if not check_state_end(s)])\n",
    "\n",
    "    state_action_values = reroll_policy(state_batch).gather(1, action_batch).squeeze()\n",
    "    \n",
    "    next_state_values = torch.zeros(bs, device=device)\n",
    "    next_state_values[non_final_mask] = moves_target(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values \n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    reroll_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    reroll_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_indices():\n",
    "    if len(indices_memory) < bs:\n",
    "        return\n",
    "    transitions = indices_memory.sample(bs)\n",
    "\n",
    "    batch = IndicesTransition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: not check_state_end(s), batch.next_state)), device=device, dtype = torch.bool)\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.indices)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state if not check_state_end(s)])\n",
    "\n",
    "    state_action_values = indices_policy(state_batch).gather(1, action_batch).squeeze()\n",
    "    \n",
    "    next_state_values = torch.zeros(bs, device=device)\n",
    "    next_state_values[non_final_mask] = moves_target(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    expected_state_action_values = next_state_values\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    indices_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    indices_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num):\n",
    "    # evaluates a model by having it play num games\n",
    "    \n",
    "    player = AIGenEightPointOne('karen')\n",
    "\n",
    "    game = DeepRLGame3DQN(player.generation)\n",
    "\n",
    "    scores = game.evaluate(num)\n",
    "\n",
    "    print('Average score: {}'.format(sum(scores) / len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_initial = 0.9\n",
    "eps_final = 0.05\n",
    "\n",
    "lr = 0.01\n",
    "bs = 64\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "total_random = 0\n",
    "total_nonrand = 0\n",
    "\n",
    "moves_policy = DQN(15).to(device)\n",
    "moves_target = DQN(15).to(device)\n",
    "moves_target.load_state_dict(moves_policy.state_dict())\n",
    "moves_target.eval()\n",
    "moves_opt = optim.SGD(moves_policy.parameters(), lr=lr)\n",
    "moves_memory = MovesReplayMemory(10000)\n",
    "\n",
    "reroll_policy = DQN(2).to(device)\n",
    "reroll_target = DQN(2).to(device)\n",
    "reroll_target.load_state_dict(reroll_policy.state_dict())\n",
    "reroll_target.eval()\n",
    "reroll_opt = optim.SGD(reroll_policy.parameters(), lr=lr)\n",
    "reroll_memory = RerollReplayMemory(10000)\n",
    "\n",
    "\n",
    "indices_policy = DQN(31).to(device)\n",
    "indices_target = DQN(31).to(device)\n",
    "indices_target.load_state_dict(indices_policy.state_dict())\n",
    "indices_target.eval()\n",
    "indices_opt = optim.SGD(indices_policy.parameters(), lr=lr)\n",
    "indices_memory = IndicesReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, gen, num):\n",
    "    moves_dict, reroll_dict, indices_dict = create_action_dicts()\n",
    "\n",
    "    global eps_decay\n",
    "    eps_decay = n\n",
    "\n",
    "    for i in range(n):\n",
    "        player = Player('karen')\n",
    "        scoresheet = player.scoresheet\n",
    "\n",
    "        turns = 0\n",
    "        while turns < 15:\n",
    "            hand = YatzyHand()\n",
    "            state = create_state(scoresheet, hand)\n",
    "\n",
    "            rerolls = 0\n",
    "\n",
    "            reroll_action = select_reroll(state, reroll_dict, i)\n",
    "            \n",
    "            while reroll_action == 0:\n",
    "                indices_action = select_indices(state, indices_dict, i)\n",
    "                hand = hand.reroll(list(indices_action))\n",
    "                rerolls += 1\n",
    "                new_state = create_state(scoresheet, hand)\n",
    "\n",
    "                reroll_memory.push(state, reroll_action, new_state)\n",
    "                indices_memory.push(state, indices_action, new_state)\n",
    "\n",
    "                reroll_action = select_reroll(new_state, reroll_dict, i) if rerolls < 2 else 1\n",
    "                state = new_state\n",
    "            \n",
    "            move_action = select_move(state, available_moves(scoresheet), i)\n",
    "            action_name = moves_dict[move_action.item()]\n",
    "            score = getattr(hand, action_name)()\n",
    "            player.update_scoresheet(action_name, score)\n",
    "\n",
    "            moves_memory.push(state, move_action, torch.tensor([score]))\n",
    "            turns += 1\n",
    "        \n",
    "        \n",
    "        optimize_moves()\n",
    "        optimize_reroll()\n",
    "        optimize_indices()\n",
    "\n",
    "        if i % TARGET_UPDATE == 0:\n",
    "            moves_target.load_state_dict(moves_policy.state_dict())\n",
    "            reroll_target.load_state_dict(reroll_policy.state_dict())\n",
    "            indices_target.load_state_dict(indices_policy.state_dict())\n",
    "        \n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Run {} done'.format(i))\n",
    "            torch.save(moves_target.state_dict(), f'dqn_models/{gen}test0/{num}/moves.pt')\n",
    "            torch.save(reroll_target.state_dict(), f'dqn_models/{gen}test0/{num}/reroll.pt')\n",
    "            torch.save(indices_target.state_dict(), f'dqn_models/{gen}test0/{num}/indices.pt')\n",
    "            print('Total random: ', total_random)\n",
    "            print('Total nonrandom: ', total_nonrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 done\n",
      "Total random:  168822\n",
      "Total nonrandom:  124776\n",
      "Run 1000 done\n",
      "Total random:  214178\n",
      "Total nonrandom:  130841\n",
      "Run 2000 done\n",
      "Total random:  258159\n",
      "Total nonrandom:  139288\n",
      "Run 3000 done\n",
      "Total random:  300977\n",
      "Total nonrandom:  150057\n",
      "Run 4000 done\n",
      "Total random:  342611\n",
      "Total nonrandom:  162841\n",
      "Run 5000 done\n",
      "Total random:  382844\n",
      "Total nonrandom:  178040\n",
      "Run 6000 done\n",
      "Total random:  421867\n",
      "Total nonrandom:  194945\n",
      "Run 7000 done\n",
      "Total random:  459512\n",
      "Total nonrandom:  213953\n",
      "Run 8000 done\n",
      "Total random:  495892\n",
      "Total nonrandom:  235115\n",
      "Run 9000 done\n",
      "Total random:  531355\n",
      "Total nonrandom:  258125\n",
      "Run 10000 done\n",
      "Total random:  565458\n",
      "Total nonrandom:  283061\n",
      "Run 11000 done\n",
      "Total random:  598519\n",
      "Total nonrandom:  309722\n",
      "Run 12000 done\n",
      "Total random:  630381\n",
      "Total nonrandom:  338141\n",
      "Run 13000 done\n",
      "Total random:  661046\n",
      "Total nonrandom:  368019\n",
      "Run 14000 done\n",
      "Total random:  690643\n",
      "Total nonrandom:  399832\n",
      "Run 15000 done\n",
      "Total random:  719217\n",
      "Total nonrandom:  433064\n",
      "Run 16000 done\n",
      "Total random:  746834\n",
      "Total nonrandom:  467939\n",
      "Run 17000 done\n",
      "Total random:  773434\n",
      "Total nonrandom:  504496\n",
      "Run 18000 done\n",
      "Total random:  799130\n",
      "Total nonrandom:  542327\n",
      "Run 19000 done\n",
      "Total random:  823714\n",
      "Total nonrandom:  581838\n"
     ]
    }
   ],
   "source": [
    "train(20000, '8.2', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2cabb87546583d089e0e7dc4e55b4bf60825fbd1f801f1fd57617645dbbf57f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
